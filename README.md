# ToyKernel

Common CUDA kernel implementations from scratch. ä»é›¶å­¦ä¹  CUDA é«˜æ€§èƒ½ç®—å­å®ç°ã€‚

## ğŸ¯ Roadmap

### ğŸŒ± Level 1: åŸºç¡€ç®—å­ï¼ˆå…¥é—¨ï¼‰

| Kernel | ç±»å‹ | Description | Status |
|--------|------|-------------|--------|
| Vector Add | Basic | å‘é‡ç›¸åŠ  | â¬œ |
| Vector Mul | Basic | å‘é‡ä¹˜æ³• | â¬œ |
| GEMV | Basic | çŸ©é˜µ-å‘é‡ä¹˜æ³• | â¬œ |
| Softmax | Basic | Softmax è®¡ç®— | â¬œ |
| Layernorm | Basic | å±‚å½’ä¸€åŒ– | â¬œ |
| RMSNorm | Basic | RMS å½’ä¸€åŒ– | â¬œ |

### ğŸš€ Level 2: æ ¸å¿ƒç®—å­ï¼ˆè¿›é˜¶ï¼‰

| Kernel | ç±»å‹ | Description | Status |
|--------|------|-------------|--------|
| GEMM (Naive) | MatMul | æœ´ç´ çŸ©é˜µä¹˜æ³• | â¬œ |
| GEMM (Tiled) | MatMul | åˆ†å—çŸ©é˜µä¹˜æ³• | â¬œ |
| GEMM (Shared Mem) | MatMul | å…±äº«å†…å­˜ä¼˜åŒ– | â¬œ |
| GEMM (Tensor Core) | MatMul | Tensor Core åŠ é€Ÿ | â¬œ |
| GeLU | Activation | æ¿€æ´»å‡½æ•° | â¬œ |
| RoPE | Position | æ—‹è½¬ä½ç½®ç¼–ç  | â¬œ |

### ğŸ”¥ Level 3: å¤§æ¨¡å‹æ ¸å¿ƒï¼ˆé«˜é˜¶ï¼‰

| Kernel | ç±»å‹ | Description | Status |
|--------|------|-------------|--------|
| FlashAttention-2 | Attention | å‰å‘ä¼ æ’­ | â¬œ |
| FlashAttention-2 (BW) | Attention | åå‘ä¼ æ’­ | â¬œ |
| PagedAttention | Memory | vLLM æ˜¾å­˜ä¼˜åŒ– | â¬œ |
| Medusa | Decoding | å¤šå¤´å¹¶è¡Œè§£ç  | â¬œ |

### âš¡ Level 4: é‡åŒ–åŠ é€Ÿï¼ˆç²¾é€šï¼‰

| Kernel | ç±»å‹ | Description | Status |
|--------|------|-------------|--------|
| W8A16 Quant | Quantization | INT8 æƒé‡é‡åŒ– | â¬œ |
| W8A16 GEMM | Quantization | INT8 é‡åŒ–ä¹˜æ³• | â¬œ |
| W4A16 Quant | Quantization | INT4 æƒé‡é‡åŒ– | â¬œ |
| W4A16 GEMM | Quantization | INT4 é‡åŒ–ä¹˜æ³• | â¬œ |
| SmoothQuant | Quantization | æ¿€æ´»å¹³æ»‘é‡åŒ– | â¬œ |
| AWQ Quant | Quantization | æ¿€æ´»æ„ŸçŸ¥é‡åŒ– | â¬œ |

---

## ğŸ“ Project Structure

```
toykernel/
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ level1/            # åŸºç¡€ç®—å­
â”‚   â”‚   â”œâ”€â”€ vector_ops.cu
â”‚   â”‚   â”œâ”€â”€ gemv.cu
â”‚   â”‚   â”œâ”€â”€ softmax.cu
â”‚   â”‚   â””â”€â”€ norm.cu
â”‚   â”œâ”€â”€ level2/           # æ ¸å¿ƒç®—å­
â”‚   â”‚   â”œâ”€â”€ gemm/
â”‚   â”‚   â”‚   â”œâ”€â”€ naive.cu
â”‚   â”‚   â”‚   â”œâ”€â”€ tiled.cu
â”‚   â”‚   â”‚   â”œâ”€â”€ shared.cu
â”‚   â”‚   â”‚   â””â”€â”€ tensor_core.cu
â”‚   â”‚   â”œâ”€â”€ activation.cu
â”‚   â”‚   â””â”€â”€ rope.cu
â”‚   â”œâ”€â”€ level3/           # å¤§æ¨¡å‹æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ flash_attention.cu
â”‚   â”‚   â””â”€â”€ paged_attention.cu
â”‚   â””â”€â”€ level4/           # é‡åŒ–
â”‚       â”œâ”€â”€ quantize.cu
â”‚       â”œâ”€â”€ dequantize.cu
â”‚       â””â”€â”€ quantized_gemm.cu
â”œâ”€â”€ include/
â”‚   â””â”€â”€ utils.cuh
â”œâ”€â”€ tests/
â”œâ”€â”€ benchmarks/
â””â”€â”€ scripts/
```

## ğŸš€ Quick Start

### ç¯å¢ƒè¦æ±‚

- CUDA Toolkit 12.x+
- CMake 3.18+
- GCC 11+
- NVIDIA GPU (sm_80+ for Tensor Cores)

### ç¼–è¯‘

```bash
mkdir build && cd build
cmake .. -DCMAKE_CUDA_ARCHITECTURES=80
make -j$(nproc)
```

### è¿è¡Œæµ‹è¯•

```bash
./tests/basic_test
./benchmarks/gemm_benchmark
```

## ğŸ“š Reference

- [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)
- [FlashDecoding Paper](https://arxiv.org/abs/2309.06169)
- [vLLM PagedAttention](https://arxiv.org/abs/2309.06180)
- [SmoothQuant](https://arxiv.org/abs/2308.15026)
- [AWQ](https://arxiv.org/abs/2306.00978)
- [CUTLASS](https://github.com/NVIDIA/cutlass)
- [GGML](https://github.com/ggerganov/ggml)

## ğŸ¤ Contributing

1. Fork this repo
2. Create your feature branch (`git checkout -b feature/xxx`)
3. Commit with proper template (`git commit` will auto-use template)
4. Push to branch
5. Open a Pull Request

## ğŸ“ License

MIT License

---

*Learning by doing. çº¸ä¸Šå¾—æ¥ç»ˆè§‰æµ…ï¼Œç»çŸ¥æ­¤äº‹è¦èº¬è¡Œã€‚*
