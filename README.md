# ToyKernel

CUDA kernel implementations from scratch. ä»é›¶å­¦ä¹  CUDA é«˜æ€§èƒ½ç®—å­å®ç°ã€‚

## ğŸ¯ Roadmap

### Level 1: åŸºç¡€ç®—å­

| Status | Kernel | Description |
|--------|--------|-------------|
| â¬œ | Vector Add | å‘é‡ç›¸åŠ  |
| â¬œ | Vector Mul | å‘é‡ä¹˜æ³• |
| â¬œ | Vector Scale | å‘é‡ç¼©æ”¾ |
| â¬œ | GEMV | çŸ©é˜µ-å‘é‡ä¹˜æ³• |
| â¬œ | Softmax | Softmax è®¡ç®— |
| â¬œ | Softmax (Warp) | Warp çº§ Softmax |
| â¬œ | Layernorm | å±‚å½’ä¸€åŒ– |
| â¬œ | RMSNorm | RMS å½’ä¸€åŒ– |
| â¬œ | GeLU | æ¿€æ´»å‡½æ•° |
| â¬œ | Swish | æ¿€æ´»å‡½æ•° |

### Level 2: GEMM

| Status | Kernel | Description |
|--------|--------|-------------|
| â¬œ | GEMV | çŸ©é˜µ-å‘é‡ä¹˜æ³• |
| â¬œ | Naive GEMM | æœ´ç´ çŸ©é˜µä¹˜æ³• |
| â¬œ | Sliced-K | K ç»´åº¦åˆ†ç‰‡ä¼˜åŒ– |
| â¬œ | Split-K | è·¨å—å¹¶è¡Œ |
| â¬œ | Persistent | å¸¸é©»çº¿ç¨‹æ¨¡å¼ |

### Level 3: å¤§æ¨¡å‹æ ¸å¿ƒ

| Status | Kernel | Description |
|--------|--------|-------------|
| â¬œ | FlashAttention-2 | å‰å‘ä¼ æ’­ |
| â¬œ | FlashDecoding | æ¨ç†è§£ç  |
| â¬œ | RoPE | æ—‹è½¬ä½ç½®ç¼–ç  |
| â¬œ | PagedAttention | vLLM æ˜¾å­˜ä¼˜åŒ– |
| â¬œ | Sparse GEMM | ç¨€ç–çŸ©é˜µä¹˜æ³• |
| â¬œ | Medusa | å¤šå¤´å¹¶è¡Œè§£ç  |

### Level 4: é‡åŒ–

| Status | Kernel | Description |
|--------|--------|-------------|
| â¬œ | W8A16 Quant | INT8 æƒé‡é‡åŒ– |
| â¬œ | W4A16 Quant | INT4 æƒé‡é‡åŒ– |
| â¬œ | W8A16 GEMM | INT8 é‡åŒ–ä¹˜æ³• |
| â¬œ | W4A16 GEMM | INT4 é‡åŒ–ä¹˜æ³• |
| â¬œ | Quantize | é‡åŒ– kernel |
| â¬œ | Quantized GEMM | é‡åŒ–çŸ©é˜µä¹˜æ³• |
| â¬œ | SmoothQuant | æ¿€æ´»å¹³æ»‘é‡åŒ– |
| â¬œ | AWQ | æ¿€æ´»æ„ŸçŸ¥é‡åŒ– |

---

## ğŸ“ Structure

```
toykernel/
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build.sh
â”‚   â”œâ”€â”€ test.sh
â”‚   â”œâ”€â”€ benchmark.sh
â”‚   â””â”€â”€ clean.sh
â”œâ”€â”€ include/
â”‚   â””â”€â”€ utils.cuh
â””â”€â”€ src/
    â”œâ”€â”€ level1/              # åŸºç¡€ç®—å­
    â”‚   â”œâ”€â”€ vector_ops.cu
    â”‚   â”œâ”€â”€ gemv.cu
    â”‚   â”œâ”€â”€ softmax.cu
    â”‚   â”œâ”€â”€ norm.cu
    â”‚   â””â”€â”€ activation.cu
    â”œâ”€â”€ level2/              # GEMM
    â”‚   â”œâ”€â”€ gemv.cu
    â”‚   â”œâ”€â”€ gemm.cu          # Naive
    â”‚   â”œâ”€â”€ sliced_k.cu
    â”‚   â”œâ”€â”€ split_k.cu
    â”‚   â””â”€â”€ persistent.cu
    â”œâ”€â”€ level3/              # LLM æ ¸å¿ƒ
    â”‚   â”œâ”€â”€ flash_attention.cu
    â”‚   â”œâ”€â”€ flash_decoding.cu
    â”‚   â”œâ”€â”€ rope.cu
    â”‚   â”œâ”€â”€ paged_attention.cu
    â”‚   â”œâ”€â”€ sparse_gemm.cu
    â”‚   â””â”€â”€ decoding.cu
    â””â”€â”€ level4/              # é‡åŒ–
        â”œâ”€â”€ weight_quant/
        â”‚   â””â”€â”€ w8a16_gemm.cu
        â”‚   â””â”€â”€ w4a16_gemm.cu
        â”œâ”€â”€ quantized_ops/
        â”‚   â”œâ”€â”€ quantize.cu
        â”‚   â””â”€â”€ quantized_gemm.cu
        â””â”€â”€ activation_quant/
            â”œâ”€â”€ smooth_quant.cu
            â””â”€â”€ awq.cu
```

## ğŸš€ Build

```bash
./scripts/build.sh
```

## ğŸ“š Ref

- [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [CUTLASS](https://github.com/NVIDIA/cutlass)
- [FlashAttention](https://arxiv.org/abs/2205.14135)
- [FlashDecoding](https://arxiv.org/abs/2309.06169)
- [vLLM](https://github.com/vllm-project/vllm)
- [SmoothQuant](https://arxiv.org/abs/2308.15026)
- [AWQ](https://arxiv.org/abs/2306.00978)

---

*Learning by doing.*
