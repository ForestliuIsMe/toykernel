# ToyKernel

Common CUDA kernel implementations from scratch. ä»é›¶å­¦ä¹  CUDA é«˜æ€§èƒ½ç®—å­å®ç°ã€‚

## ğŸ¯ Roadmap

### ğŸŒ± Level 1: åŸºç¡€ç®—å­ï¼ˆå…¥é—¨ï¼‰

| Kernel | ç±»å‹ | Description | Status |
|--------|------|-------------|--------|
| Vector Add | Basic | å‘é‡ç›¸åŠ  | â¬œ |
| Vector Mul | Basic | å‘é‡ä¹˜æ³• | â¬œ |
| GEMV | Basic | çŸ©é˜µ-å‘é‡ä¹˜æ³• | â¬œ |
| Softmax | Basic | Softmax è®¡ç®— | â¬œ |
| Layernorm | Basic | å±‚å½’ä¸€åŒ– | â¬œ |
| RMSNorm | Basic | RMS å½’ä¸€åŒ– | â¬œ |

### ğŸš€ Level 2: GEMM æ ¸å¿ƒï¼ˆè¿›é˜¶ï¼‰

#### 2.1 Naive GEMMï¼ˆå…¥é—¨ï¼‰
| Kernel | ç±»å‹ | Description | Status |
|--------|------|-------------|--------|
| GEMM (Row-Major) | Naive | æœ´ç´ è¡Œä¼˜å…ˆçŸ©é˜µä¹˜æ³• | â¬œ |
| GEMM (Col-Major) | Naive | æœ´ç´ åˆ—ä¼˜å…ˆçŸ©é˜µä¹˜æ³• | â¬œ |
| GEMV | Naive | çŸ©é˜µ-å‘é‡ä¹˜æ³•ï¼ˆGEMM ç®€åŒ–ç‰ˆï¼‰ | â¬œ |

#### 2.2 Sliced-Kï¼ˆåˆ†ç‰‡ç­–ç•¥ï¼‰
| Kernel | ç±»å‹ | Description | Status |
|--------|------|-------------|--------|
| GEMM (Sliced-K Basic) | Sliced-K | æŒ‰ K ç»´åº¦åˆ†ç‰‡ï¼Œå‡å°‘å…±äº«å†…å­˜ | â¬œ |
| GEMM (Sliced-K Warp) | Sliced-K | Warp çº§åˆ†ç‰‡å¹¶è¡Œ | â¬œ |
| GEMM (Sliced-K TensorCore) | Sliced-K | Tensor Core + Sliced-K æ··åˆ | â¬œ |

#### 2.3 Split-Kï¼ˆè·¨å—å¹¶è¡Œï¼‰
| Kernel | ç±»å‹ | Description | Status |
|--------|------|-------------|--------|
| GEMM (Split-K Basic) | Split-K | K ç»´åº¦è·¨çº¿ç¨‹å—å¹¶è¡Œ | â¬œ |
| GEMM (Split-K Reduce) | Split-K | Split-K + è·¨å—å½’çº¦ | â¬œ |
| GEMM (Split-K Async) | Split-K | å¼‚æ­¥æ‰§è¡Œä¼˜åŒ– | â¬œ |

#### 2.4 Persistentï¼ˆå¸¸é©»çº¿ç¨‹ï¼‰
| Kernel | ç±»å‹ | Description | Status |
|--------|------|-------------|--------|
| GEMM (Persistent Basic) | Persistent | çº¿ç¨‹å¸¸é©»ï¼Œæ‰¹å¤„ç†å¤ç”¨ | â¬œ |
| GEMM (Persistent Stream) | Persistent | å¤šæµå¹¶å‘æ‰§è¡Œ | â¬œ |
| GEMM (Persistent TensorCore) | Persistent | Tensor Core + Persistent æ¨¡å¼ | â¬œ |

**GEMM å­¦ä¹ è·¯çº¿ï¼š**
```
Naive â†’ Sliced-K â†’ Split-K â†’ Persistent
(ç†è§£åŸç†) â†’ (å†…å­˜ä¼˜åŒ–) â†’ (å¹¶è¡Œæ‰©å±•) â†’ (æè‡´æ€§èƒ½)
```

### ğŸ”¥ Level 3: å¤§æ¨¡å‹æ ¸å¿ƒï¼ˆé«˜é˜¶ï¼‰

| Kernel | ç±»å‹ | Description | Status |
|--------|------|-------------|--------|
| FlashAttention-2 | Attention | å‰å‘ä¼ æ’­ | â¬œ |
| FlashAttention-2 (BW) | Attention | åå‘ä¼ æ’­ | â¬œ |
| PagedAttention | Memory | vLLM æ˜¾å­˜ä¼˜åŒ– | â¬œ |
| Medusa | Decoding | å¤šå¤´å¹¶è¡Œè§£ç  | â¬œ |

### âš¡ Level 4: é‡åŒ–åŠ é€Ÿï¼ˆç²¾é€šï¼‰

| Kernel | ç±»å‹ | Description | Status | å‚è€ƒ |
|--------|------|-------------|--------|------|
| W8A16 Quant | Quantization | INT8 æƒé‡é‡åŒ–ï¼ŒFP16 è®¡ç®— | â¬œ | AWQ, GPTQ |
| W8A16 GEMM | Quantization | INT8 é‡åŒ–çŸ©é˜µä¹˜æ³• | â¬œ | BitBLAS, TensorRT |
| W4A16 Quant | Quantization | INT4 æƒé‡é‡åŒ–ï¼ŒFP16 è®¡ç®— | â¬œ | GGUF, AWQ |
| W4A16 GEMM | Quantization | INT4 é‡åŒ–çŸ©é˜µä¹˜æ³• | â¬œ | GGML, AWQ |
| W4A4 Quant | Quantization | INT4 æƒé‡ + INT4 æ¿€æ´» | â¬œ | QLoRA, GPTQ |
| SmoothQuant | Quantization | æ¿€æ´»å¹³æ»‘ï¼Œè¿ç§»é‡åŒ–éš¾åº¦ | â¬œ | Microsoft |
| Dequantize | Quantization | åé‡åŒ– kernel | â¬œ | é€šç”¨ |
| KV Cache Quant | Quantization | KV cache INT8/INT4 é‡åŒ– | â¬œ | vLLM, SqueezeLLM |

**é‡åŒ–ç²¾åº¦å¯¹æ¯”ï¼š**
```
FP16 > W8A16 > W4A16 > W4A4
æ˜¾å­˜å ç”¨ï¼š1x > 0.5x > 0.25x > 0.125x
```

---

## ğŸ“ Project Structure

```
toykernel/
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ level1/            # åŸºç¡€ç®—å­
â”‚   â”‚   â”œâ”€â”€ vector_ops.cu
â”‚   â”‚   â”œâ”€â”€ gemv.cu
â”‚   â”‚   â”œâ”€â”€ softmax.cu
â”‚   â”‚   â””â”€â”€ norm.cu
â”‚   â”œâ”€â”€ level2/           # GEMM æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ gemm/
â”‚   â”‚   â”‚   â”œâ”€â”€ naive/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ row_major.cu
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ col_major.cu
â”‚   â”‚   â”‚   â”œâ”€â”€ sliced_k/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ basic.cu
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ warp.cu
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ tensor_core.cu
â”‚   â”‚   â”‚   â”œâ”€â”€ split_k/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ basic.cu
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ reduce.cu
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ async.cu
â”‚   â”‚   â”‚   â””â”€â”€ persistent/
â”‚   â”‚   â”‚       â”œâ”€â”€ basic.cu
â”‚   â”‚   â”‚       â”œâ”€â”€ stream.cu
â”‚   â”‚   â”‚       â””â”€â”€ tensor_core.cu
â”‚   â”‚   â”œâ”€â”€ activation.cu
â”‚   â”‚   â””â”€â”€ rope.cu
â”‚   â”œâ”€â”€ level3/           # å¤§æ¨¡å‹æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ flash_attention.cu
â”‚   â”‚   â””â”€â”€ paged_attention.cu
â”‚   â””â”€â”€ level4/           # é‡åŒ–
â”‚       â”œâ”€â”€ quantize.cu
â”‚       â”œâ”€â”€ dequantize.cu
â”‚       â””â”€â”€ quantized_gemm.cu
â”œâ”€â”€ include/
â”‚   â””â”€â”€ utils.cuh
â”œâ”€â”€ tests/
â”œâ”€â”€ benchmarks/
â””â”€â”€ scripts/
```

## ğŸš€ Quick Start

### ç¯å¢ƒè¦æ±‚

- CUDA Toolkit 12.x+
- CMake 3.18+
- GCC 11+
- NVIDIA GPU (sm_80+ for Tensor Cores)

### ç¼–è¯‘

```bash
mkdir build && cd build
cmake .. -DCMAKE_CUDA_ARCHITECTURES=80
make -j$(nproc)
```

### è¿è¡Œæµ‹è¯•

```bash
./tests/basic_test
./benchmarks/gemm_benchmark
```

## ğŸ“š Reference

- [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [CUTLASS GEMM](https://github.com/NVIDIA/cutlass)
- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)
- [FlashDecoding Paper](https://arxiv.org/abs/2309.06169)
- [vLLM PagedAttention](https://arxiv.org/abs/2309.06180)
- [SmoothQuant](https://arxiv.org/abs/2308.15026)
- [AWQ](https://arxiv.org/abs/2306.00978)
- [GGML](https://github.com/ggerganov/ggml)

## ğŸ¤ Contributing

1. Fork this repo
2. Create your feature branch (`git checkout -b feature/xxx`)
3. Commit with proper template (`git commit` will auto-use template)
4. Push to branch
5. Open a Pull Request

## ğŸ“ License

MIT License

---

*Learning by doing. çº¸ä¸Šå¾—æ¥ç»ˆè§‰æµ…ï¼Œç»çŸ¥æ­¤äº‹è¦èº¬è¡Œã€‚*
